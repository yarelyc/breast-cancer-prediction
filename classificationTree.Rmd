---
title: "Predicting Breast Cancer using Decision Trees"
author: "Hugo Argueta"
date: "May 9, 2017"
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(prompt=TRUE, comment="", echo=TRUE)
```

```{r}
library(rpart)
library(rpart.plot)
```

### Load data

```{r}
set.seed(135)

dat = read.csv("/Users/juice/Documents/CST463/final-project/breast-cancer-prediction/data.csv")
dat$X = NULL
```

### Split dataset into training set and testing set

```{r}
num_tr = sample(1:nrow(dat), floor(nrow(dat) * .75))
tr_dat = dat[num_tr,]
te_dat = dat[-num_tr,]
```

### Data exploration

Here, we are plotting texture_worst by radius_worst and displaying the occurrences of breast cancer in red, and the occurrences of no breast cancer in green for each point in our plot. This plot shows that there is a boundary line at around 16. This may indicate that these two features will be useful in predicting breast cancer.

```{r}
plot(dat$texture_worst, dat$radius_worst, col = "green", main = "Texture Worst by Radius Worst", xlab = "texture worst", ylab = "radius worst", pch=16)
malignant = dat$diagnosis == "M"
points(dat[malignant, ]$texture_worst, dat[malignant, ]$radius_worst, col = "red", pch=16)
text_rad_fit = rpart(diagnosis ~ texture_worst + radius_worst, data=tr_dat, method="class")
split = text_rad_fit$splits[1, "index"]
abline(h=split, lty=2, col='blue')
```

Plot texture_worst by concave.points_worst

```{r}
plot(dat$texture_worst, dat$concave.points_worst, col = "green", main = "Texture Worst by Concave.Points Worst", xlab = "texture worst", ylab = "concave.points worst", pch=16)
malignant = dat$diagnosis == "M"
points(dat[malignant, ]$texture_worst, dat[malignant, ]$concave.points_worst, col = "red", pch=16)
text_conc_fit = rpart(diagnosis ~ texture_worst + concave.points_worst, data=tr_dat, method="class")
split = text_conc_fit$splits[1, "index"]
abline(h=split, lty=2, col='blue')
```

### Create Decision Tree classifier

We are creating a decision tree classifier to predict the diagnosis feature based on the features radius_worst, concave.points_worst, and texture_worst.

```{r}
fit = rpart(diagnosis ~ radius_worst + concave.points_worst + texture_worst, data=tr_dat, method="class")

prp(fit, extra=106, varlen=-10, main="Classification Tree for Breast Cancer", box.col=c("green", "red")[fit$frame$yval])
```

From the confusion matrix for actuals vs. predicted, we can see that we have low false positives and no false negatives.

```{r}
predicts = predict(fit, te_dat, type="class")
actuals = te_dat$diagnosis

conf_mtx = table(actuals, predicts)
conf_mtx
```

We calculate the classifier's accuracy

```{r}
round(mean(predicts == actuals), 3)
```

Here, we calculate the classifier's precision

```{r}
conf_mtx[2,2] / (conf_mtx[2,2] + conf_mtx[1,2])
```

Here, we calculate the classifier's recall

```{r}
conf_mtx[2,2] / (conf_mtx[2,2] + conf_mtx[2,1])
```

### Learning Curve for Decision Tree Classification

From this learning curve, we can see that this classifier has low bias and low variance because the training errors and the test error lines converge as the training set size increases.

```{r echo=FALSE}
te_errs = c()
tr_errs = c()
te_actual = te_dat$diagnosis
tr_sizes = seq(100, nrow(tr_dat), length.out=10)
for (tr_size in tr_sizes) {
  tr_dat1 = tr_dat[1:tr_size,]
  tr_actual = tr_dat1$diagnosis
  fit = rpart(diagnosis ~ radius_worst + concave.points_worst + texture_worst, data=tr_dat1, method="class")
  # error on training set
  tr_predicted = predict(fit, tr_dat1, type="class")
  tr_predicted
  err = sum(tr_actual != tr_predicted)/length(tr_predicted)
  tr_errs = c(tr_errs, err)
 
  # error on test set
  te_predicted = predict(fit, te_dat, type="class")
  err = sum(te_actual != te_predicted)/length(te_predicted)
  te_errs = c(te_errs, err)
  te_errs
}

plot(range(tr_sizes), range(te_errs, tr_errs), type='n', xlab='training set size', ylab='error')

lines(tr_sizes, tr_errs, col='green')
lines(tr_sizes, te_errs, col='red')
```
